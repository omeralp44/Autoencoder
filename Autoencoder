QUESTION 1 PART A
import numpy as np
import h5py
import matplotlib.pyplot as plt

f = h5py.File('data1.h5', 'r')
list(f.keys())

dset = f ["data"]
dset = np.array(dset)
print(dset.shape)

def toGrayScale(M):
  #RED
  M_RED = M[:,0] * 0.2126  
  #GREEN
  M_GREEN = M[:,1] * 0.7152
  #BLUE
  M_BLUE = M[:,2] * 0.0722

  Y = M_RED + M_GREEN + M_BLUE
  return Y

def normalization(M):
  M0 = M
  mean = np.mean(M0)
  std = np.std(M0, dtype=np.float64)
  
  M = (M0 - mean) / (3*std)
  # mapping
  M1 = ((0.8) * (M - np.min(M)) / (np.max(M) - np.min(M))) + 0.1  
  return M1

#Get the grayscale format
gscale = toGrayScale(dset)

samples = np.zeros((200,16,16))
for i in range (200):
  rand = np.random.randint(10240)
  samples[i] = gscale[rand]

plt.figure(figsize = (20,20))
for i in range (200):
  plt.subplot(20,10 , i+1)
  plt.imshow(samples[i])

samplesNorm = normalization(samples)

plt.figure(figsize = (20,20))
for i in range (200):
  plt.subplot(20,10 , i+1)
  plt.imshow(samplesNorm[i], cmap="gray")


QUESTION 1: PART B
X = gscale
X = normalization(gscale)
X = X.reshape((10240,256))
X = X.T

print(X.shape)

def layer_sizes(X):
  n_x = X.shape[0]
  n_y = X.shape[0]
  n_h = 64
  return n_x, n_h, n_y 
n_x, n_h, n_y = layer_sizes(X)
print(layer_sizes(X)[1])

def initialize_parameters(n_x, n_h, n_y):
  w0 = np.sqrt(6/(n_h + n_y))
  
  b1 = np.random.uniform(-w0,w0, size = (n_h,1))
  b2 = np.random.uniform(-w0,w0, size = (n_y,1))
  W1 = np.random.uniform(-w0,w0, size = (n_h,n_x))
  W2 = np.random.uniform(-w0,w0, size = (n_y,n_h))
  
  
  parameters = {  "W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2  }
  return parameters
parameters = initialize_parameters(n_x, n_h, n_y) 

def sigmoid(x):
  s = 1/(1+np.exp(-x))
  return s

def forward_propagation(X, parameters):
   W1 = parameters["W1"]
   W2 = parameters["W2"]
   b1 = parameters["b1"]
   b2 = parameters["b2"]
   
  
   Z1 = np.dot(W1,X) + b1
   A1 = sigmoid(Z1)
   Z2 = W2.dot(A1) + b2
   A2 = sigmoid(Z2)  
  
   cache = { "Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2 }
   return A2, cache
A2, cache = forward_propagation(X, parameters)
Y = X

def KL_divergence(p,pb):
  inn = p * np.log(p/pb)
  inn2 = ((1-p)*np.log((1-p)/(1-pb)))
  d_KL = np.sum(inn + inn2)
  return d_KL

def calculate_cost(X,Y,parameters,cache):
  N = Y.shape[1]
  first_term = (1/(2*N)) * np.sum(np.square((X-Y)))
  
  A1 = cache["A1"]
  
  W1 = parameters["W1"]
  W2 = parameters["W2"]
  lam = 0.001
  second_term = (lam/2) * (np.sum(np.square(W1)) + np.sum(np.square(W2)))
  
  p = 0.1
  bet = 0.1
  pb_vector = np.sum(A1, axis = 1, keepdims = True) / N
  d_KL = KL_divergence(p,pb_vector)
  third_term = bet * d_KL
  
  
  
  cost = first_term + second_term + third_term
  cost = float(np.squeeze(cost))
  
  return cost

cost = calculate_cost(X,A2,parameters,cache)
print(cost)

def backward_propagation(parameters, cache, X):
    m = X.shape[1]

    
    W1 = parameters["W1"]
    W2 = parameters["W2"]
      
    A1 = cache["A1"]
    A2 = cache["A2"]
    
    dZ2 = A2 - X
    dZ1 = np.multiply(np.dot(np.transpose(W2), dZ2) , (A1 * (1 - A1)))
    dW2 = np.dot(dZ2, np.transpose(A1)) / m
    db2 = np.sum(dZ2, axis = 1, keepdims = True) / m
    
    dW1 = np.dot(dZ1, np.transpose(X)) /m
    db1 = np.sum(dZ1, axis = 1, keepdims = True) / m
    
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}
    
    return grads

def update_parameters(parameters, grads, learning_rate):
   
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    b1 = parameters["b1"]
    b2 = parameters["b2"]

    dW1 = grads["dW1"]
    dW2 = grads["dW2"]
    db1 = grads["db1"]
    db2 = grads["db2"]
 
    W1 = W1 - learning_rate * dW1
    W2 = W2 - learning_rate * dW2
    b1 = b1 - learning_rate * db1
    b2 = b2 - learning_rate * db2
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters

# NN_model
def nn_model(X, Y, n_h, learning_rate, num_iterations, print_cost=False):
    n_x = layer_sizes(X)[0]
    n_y = layer_sizes(X)[2]
    plot = []
    # Initialize parameters
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    # Loop (gradient descent)
    for i in range(0, num_iterations + 1):       
        A2, cache = forward_propagation(X, parameters)
       
        cost = calculate_cost(X, A2, parameters, cache)
        
        grads = backward_propagation(parameters, cache, X)
       
        parameters = update_parameters(parameters, grads, learning_rate)
        
        plot.append(cost)
        if print_cost and i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))
          
    
    return parameters,plot

parameters,plot = nn_model(X, Y, 60, 0.01, num_iterations= 1000, print_cost=True)

plt.figure(figsize=(10,10)) 
plt.plot(plot)
plt.xlabel("Number of iterations")
plt.ylabel("Cost")

print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))


QUESTION 1 PART C
W1 = parameters["W1"]
W1 = W1.reshape((60,16,16)) #W1 = W1.reshape((n_h,16,16))

plt.figure(figsize = ((30,30)) , dpi = 80)
for i in range (len(W1)):
  plt.subplot(10, len(W1)/6, i+1) # plt.subplot(y, len(W1)/y, i+1) y = n_h / x 
  plt.imshow(W1[i])
